# raw データの自動削除について

## 概要
`raw/` フォルダには、クローラーが取得したすべての生データがJSON形式で保存されています。
これらのデータは時間の経過とともに蓄積され、ストレージを圧迫する可能性があります。

## 削除ポリシー

### 推奨設定
- **保持期間**: 30日間
- **実行タイミング**: 毎日のクロール実行後に自動削除
- **理由**:
  - デバッグや再処理に必要な期間を確保
  - 1ヶ月あれば、データの問題や不具合の発見・対処が可能
  - ストレージ使用量を適切に管理（月8MB程度）

### データの性質
1. **バックアップデータ**: Supabaseに正常に保存されたデータの生バックアップ
2. **デバッグ用途**: パース処理の問題調査時に参照
3. **再処理用**: データ構造変更時の移行作業で使用可能

## 自動削除の仕組み

### GitHub Actions による自動実行（実装済み）
`.github/workflows/crawl.yml` に組み込まれており、毎日のクロール実行後に30日以上前のデータを自動削除します。

```yaml
- name: Cleanup old raw data
  run: |
    echo "Cleaning up raw data older than 30 days..."
    python scripts/cleanup_raw.py --days 30
```

### 手動実行
必要に応じて手動でもスクリプトを実行できます：

```bash
# 30日以上前のデータを削除（デフォルト）
python scripts/cleanup_raw.py

# 60日以上前のデータを削除
python scripts/cleanup_raw.py --days 60

# 削除対象を確認のみ（実際には削除しない）
python scripts/cleanup_raw.py --dry-run
```

## 注意事項

1. **削除前の確認**: 本番環境では必ず `--dry-run` で削除対象を確認
2. **バックアップ**: 重要なデータは別途バックアップを検討
3. **保持期間の調整**: 組織のニーズに応じて保持期間を調整
4. **ログの確認**: 自動実行時はログファイルで削除結果を確認

## カスタマイズ

保持期間の目安:
- **7日**: ストレージ重視、問題がすぐに発覚する環境
- **30日**: バランス重視（推奨）
- **60日**: データ保全重視、四半期レポート等で過去データ参照が必要
- **90日**: コンプライアンス要件がある場合

必要に応じて、特定のソース（重要度の高いもの）のみ長期保存するような
より高度な削除ポリシーも実装可能です。