#!/usr/bin/env python3
"""
HTMLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾å¿œã®æƒ…å ±æºè¿½åŠ 
RSSãŒç„¡ã„ã‚µã‚¤ãƒˆã§ã‚‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
"""
import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime
import re
from typing import List, Dict, Optional
from supabase import create_client, Client

# Supabaseæ¥ç¶š
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

class HTMLSourceAnalyzer:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def analyze_news_page(self, url: str) -> Dict:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®æ§‹é€ ã‚’åˆ†æ"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º
            article_links = self.find_article_links(soup, url)
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º
            pagination = self.find_pagination(soup, url)
            
            # æ—¥ä»˜æƒ…å ±æ¤œå‡º
            date_pattern = self.detect_date_pattern(soup)
            
            return {
                'url': url,
                'article_count': len(article_links),
                'article_links': article_links[:5],  # ã‚µãƒ³ãƒ—ãƒ«5å€‹
                'pagination': pagination,
                'date_pattern': date_pattern,
                'scraping_feasible': len(article_links) > 0
            }
            
        except Exception as e:
            return {
                'url': url,
                'error': str(e),
                'scraping_feasible': False
            }
    
    def find_article_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º"""
        articles = []
        
        # ã‚ˆãã‚ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
        selectors = [
            'a[href*="/news/"]',
            'a[href*="/press/"]', 
            'a[href*="/topics/"]',
            '.news-item a',
            '.news-list a',
            '.article-title a',
            '.post-title a',
            'h3 a', 'h4 a',  # è¦‹å‡ºã—ã®ãƒªãƒ³ã‚¯
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            
            for link in links:
                href = link.get('href')
                if href:
                    full_url = urljoin(base_url, href)
                    title = link.get_text().strip()
                    
                    if title and len(title) > 10:  # ååˆ†ãªé•·ã•ã®ã‚¿ã‚¤ãƒˆãƒ«
                        articles.append({
                            'title': title,
                            'url': full_url,
                            'selector': selector
                        })
        
        # é‡è¤‡é™¤å»
        seen_urls = set()
        unique_articles = []
        for article in articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        return unique_articles
    
    def find_pagination(self, soup: BeautifulSoup, base_url: str) -> Dict:
        """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æ¤œå‡º"""
        pagination = {'has_pagination': False}
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        pagination_selectors = [
            '.pagination a',
            '.pager a', 
            'a[href*="page="]',
            'a[href*="&page="]',
            '.next-page',
            '.page-next'
        ]
        
        for selector in pagination_selectors:
            elements = soup.select(selector)
            if elements:
                pagination['has_pagination'] = True
                pagination['selector'] = selector
                pagination['sample_links'] = [
                    urljoin(base_url, elem.get('href', '')) 
                    for elem in elements[:3]
                ]
                break
        
        return pagination
    
    def detect_date_pattern(self, soup: BeautifulSoup) -> Dict:
        """æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        date_patterns = {
            'detected': False,
            'patterns': []
        }
        
        # æ—¥ä»˜ã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢
        date_regex_patterns = [
            r'\d{4}[/-]\d{1,2}[/-]\d{1,2}',  # 2024/01/01
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',      # 2024å¹´1æœˆ1æ—¥
            r'\d{1,2}/\d{1,2}/\d{4}',         # 01/01/2024
        ]
        
        text_content = soup.get_text()
        
        for pattern in date_regex_patterns:
            matches = re.findall(pattern, text_content)
            if matches:
                date_patterns['detected'] = True
                date_patterns['patterns'].append({
                    'regex': pattern,
                    'samples': matches[:3]
                })
        
        return date_patterns

def test_html_sources():
    """HTMLå¯¾å¿œå€™è£œã®ãƒ†ã‚¹ãƒˆ"""
    analyzer = HTMLSourceAnalyzer()
    
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸
    test_sources = [
        {
            'name': 'Toray Industries',
            'url': 'https://www.toray.co.jp/news/?language=JA&region=JAPAN',
            'expected_keywords': ['CFRP', 'ç‚­ç´ ç¹Šç¶­', 'è¤‡åˆææ–™']
        },
        {
            'name': 'Hexcel',
            'url': 'https://www.hexcel.com/news-events/news',
            'expected_keywords': ['composites', 'carbon fiber', 'prepreg']
        },
        {
            'name': 'SGL Carbon', 
            'url': 'https://www.sglcarbon.com/en/press-center/press-releases/',
            'expected_keywords': ['carbon', 'composites', 'materials']
        }
    ]
    
    print("ğŸ” HTML ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾å¿œå¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    results = []
    
    for source in test_sources:
        print(f"\nğŸ“° {source['name']}")
        print(f"URL: {source['url']}")
        
        analysis = analyzer.analyze_news_page(source['url'])
        results.append({**source, 'analysis': analysis})
        
        if analysis.get('scraping_feasible'):
            print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½")
            print(f"   è¨˜äº‹æ•°: {analysis['article_count']}")
            print(f"   ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³: {analysis['pagination']['has_pagination']}")
            print(f"   æ—¥ä»˜æ¤œå‡º: {analysis['date_pattern']['detected']}")
            
            if analysis['article_links']:
                print("   ã‚µãƒ³ãƒ—ãƒ«è¨˜äº‹:")
                for article in analysis['article_links'][:2]:
                    print(f"     - {article['title'][:50]}...")
        else:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å›°é›£")
            if 'error' in analysis:
                print(f"   ã‚¨ãƒ©ãƒ¼: {analysis['error']}")
    
    return results

def add_html_sources_to_db(sources: List[Dict]):
    """HTMLå¯¾å¿œã‚½ãƒ¼ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ """
    print("\nğŸ“ HTMLå¯¾å¿œã‚½ãƒ¼ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ")
    
    for source in sources:
        if source['analysis'].get('scraping_feasible'):
            try:
                # parser ã‚’ 'html' ã«è¨­å®š
                source_data = {
                    'name': f"{source['name']} (HTML)",
                    'urls': [source['url']],
                    'parser': 'html',  # RSS ã®ä»£ã‚ã‚Šã« HTML
                    'acquisition_mode': 'auto',
                    'category': 'news',
                    'domain': urlparse(source['url']).netloc,
                    'country_code': 'US' if 'hexcel' in source['url'] else 'JP',
                    'relevance': 8
                }
                
                result = supabase.table("sources").insert(source_data).execute()
                print(f"âœ… è¿½åŠ å®Œäº†: {source['name']}")
                
            except Exception as e:
                print(f"âŒ è¿½åŠ ã‚¨ãƒ©ãƒ¼ ({source['name']}): {e}")

def create_html_parser_extension():
    """HTML ãƒ‘ãƒ¼ã‚µãƒ¼æ‹¡å¼µã‚’crawl.pyã«è¿½åŠ ã™ã‚‹ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ"""
    
    html_parser_code = '''
# HTML ãƒ‘ãƒ¼ã‚µãƒ¼æ‹¡å¼µ (crawl.py ã«è¿½åŠ )

def parse_html_news_page(url: str, config: dict) -> List[Dict]:
    """HTML ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡º"""
    try:
        response = session.get(url, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = []
        
        # è¨˜äº‹ãƒªãƒ³ã‚¯ã®æ¤œå‡ºï¼ˆã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        selectors = [
            'a[href*="/news/"]',
            'a[href*="/press/"]',
            '.news-item a',
            '.news-list a',
            'h3 a', 'h4 a'
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            
            for link in links:
                href = link.get('href')
                title = link.get_text().strip()
                
                if href and title and len(title) > 10:
                    full_url = urljoin(url, href)
                    
                    # æ—¥ä»˜æŠ½å‡ºï¼ˆè¦ªè¦ç´ ã‹ã‚‰ï¼‰
                    date_elem = link.find_parent().find(text=re.compile(r'\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}'))
                    published_at = date_elem if date_elem else None
                    
                    articles.append({
                        'title': title,
                        'link': full_url,
                        'published': published_at,
                        'id': full_url,  # RSS ã® 'id' ç›¸å½“
                        'summary': ''  # å¾Œã§è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
                    })
        
        return articles[:20]  # æœ€æ–°20ä»¶ã«åˆ¶é™
        
    except Exception as e:
        print(f"HTMLè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return []

# crawl.py ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ :
# if src.get("parser") == "html":
#     entries = parse_html_news_page(feed_url, cfg)
# else:
#     entries = fetch_and_parse(feed_url, cfg)
'''
    
    with open('html_parser_extension.py', 'w', encoding='utf-8') as f:
        f.write(html_parser_code)
    
    print("ğŸ“„ HTML ãƒ‘ãƒ¼ã‚µãƒ¼æ‹¡å¼µã‚³ãƒ¼ãƒ‰ç”Ÿæˆ: html_parser_extension.py")

def main():
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    if not os.getenv("SUPABASE_URL") or not os.getenv("SUPABASE_KEY"):
        print("âŒ ã‚¨ãƒ©ãƒ¼: SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    print("ğŸŒ HTML ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ")
    print("RSSãŒç„¡ã„ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—")
    print()
    
    # HTMLã‚½ãƒ¼ã‚¹ã®åˆ†æ
    results = test_html_sources()
    
    # å®Ÿè¡Œå¯èƒ½ãªã‚½ãƒ¼ã‚¹ã‚’DBã«è¿½åŠ ã™ã‚‹ã‹ç¢ºèª
    feasible_sources = [s for s in results if s['analysis'].get('scraping_feasible')]
    
    if feasible_sources:
        print(f"\nğŸ¯ {len(feasible_sources)} å€‹ã®ã‚µã‚¤ãƒˆã§HTMLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½")
        
        # HTML ãƒ‘ãƒ¼ã‚µãƒ¼æ‹¡å¼µã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
        create_html_parser_extension()
        
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. html_parser_extension.py ã®ã‚³ãƒ¼ãƒ‰ã‚’ crawl.py ã«çµ±åˆ")
        print("2. BeautifulSoup4 ã‚’ requirements.txt ã«è¿½åŠ ")
        print("3. HTMLå¯¾å¿œã‚½ãƒ¼ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ")
        
    else:
        print("âŒ HTMLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½ãªã‚µã‚¤ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main()