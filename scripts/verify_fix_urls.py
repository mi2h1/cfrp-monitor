#!/usr/bin/env python3
"""
sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã®urlsã‚’æ¤œè¨¼ã—ã€å•é¡ŒãŒã‚ã‚‹URLã‚’ä¿®æ­£
- ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯URLã®æ¤œå‡º
- ä»£æ›¿URLï¼ˆ.co.jpç­‰ï¼‰ã®æ¢ç´¢
- RSS/ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®ä¿®æ­£
"""
import os
import requests
from urllib.parse import urlparse
import time
from typing import Dict, List, Optional, Tuple
from supabase import create_client, Client

# Supabaseæ¥ç¶š
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

class URLVerifier:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ä»£æ›¿å€™è£œ
        self.domain_alternatives = {
            'teijin.com': ['teijin.co.jp'],
            'toray.com': ['toray.co.jp'],
            'hexcel.com': ['hexcel.com'],  # ç±³å›½ä¼æ¥­ãªã®ã§.comæ­£ã—ã„ã¯ãš
            'chemicaldaily.co.jp': ['chemicaldaily.co.jp'],
            'jeccomposites.com': ['jeccomposites.com'],
            'solvay.com': ['solvay.com'],
            'sglcarbon.com': ['sglcarbon.com', 'sglcarbon.de']
        }
        
        # RSS/ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ‘ã‚¹å€™è£œ
        self.feed_paths = [
            '/rss.xml', '/feed.xml', '/rss/', '/feed/',
            '/news/rss.xml', '/news/feed.xml', '/news/rss/', '/news/feed/',
            '/press/rss.xml', '/press/feed.xml',
            '/en/news/rss.xml', '/en/rss.xml',
            '/jp/news/rss.xml', '/jp/rss.xml'
        ]
    
    def test_url(self, url: str) -> Tuple[bool, int, str]:
        """URLã‚’ãƒ†ã‚¹ãƒˆã—ã¦çŠ¶æ…‹ã‚’è¿”ã™"""
        try:
            response = self.session.head(url, timeout=10, allow_redirects=True)
            return True, response.status_code, response.url
        except Exception as e:
            return False, 0, str(e)
    
    def find_working_feed(self, base_domain: str) -> Optional[str]:
        """æœ‰åŠ¹ãªãƒ•ã‚£ãƒ¼ãƒ‰URLã‚’æ¢ç´¢"""
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ä»£æ›¿å€™è£œã‚’è©¦è¡Œ
        domains_to_try = self.domain_alternatives.get(base_domain, [base_domain])
        
        for domain in domains_to_try:
            print(f"    ğŸ” {domain} ã§æ¤œç´¢ä¸­...")
            
            for path in self.feed_paths:
                test_url = f"https://{domain}{path}"
                
                success, status_code, final_url = self.test_url(test_url)
                
                if success and status_code == 200:
                    # Content-Typeã‚‚ç¢ºèª
                    try:
                        response = self.session.get(test_url, timeout=8)
                        content_type = response.headers.get('content-type', '').lower()
                        
                        # XML/RSSã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ç¢ºèª
                        if any(ct in content_type for ct in ['xml', 'rss', 'atom']) or \
                           any(keyword in response.text[:500].lower() for keyword in ['<rss', '<feed', '<channel']):
                            print(f"    âœ… ç™ºè¦‹: {test_url}")
                            return test_url
                    except:
                        continue
                
                # çŸ­ã„é–“éš”ã§æ¬¡ã‚’ãƒ†ã‚¹ãƒˆ
                time.sleep(0.5)
        
        return None
    
    def get_all_sources(self) -> List[Dict]:
        """å…¨sourcesã‚’å–å¾—"""
        try:
            result = supabase.table("sources").select("id, name, domain, urls").execute()
            return result.data
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def verify_and_fix_all_urls(self):
        """å…¨URLã®æ¤œè¨¼ã¨ä¿®æ­£"""
        sources = self.get_all_sources()
        
        print(f"ğŸ” {len(sources)} å€‹ã®ã‚½ãƒ¼ã‚¹ã®URLæ¤œè¨¼é–‹å§‹")
        print("=" * 60)
        
        broken_count = 0
        fixed_count = 0
        
        for i, source in enumerate(sources, 1):
            source_id = source['id']
            name = source['name']
            domain = source['domain']
            urls = source.get('urls', [])
            
            print(f"\n[{i}/{len(sources)}] {name} ({domain})")
            
            if not urls:
                print("    âš ï¸ URLãŒæœªè¨­å®š")
                continue
            
            all_working = True
            fixed_urls = []
            
            for url in urls:
                print(f"    ãƒ†ã‚¹ãƒˆä¸­: {url}")
                success, status_code, final_url = self.test_url(url)
                
                if success and status_code == 200:
                    print(f"    âœ… OK ({status_code})")
                    fixed_urls.append(url)
                else:
                    print(f"    âŒ NG ({status_code}) - {url}")
                    broken_count += 1
                    all_working = False
                    
                    # ä»£æ›¿URLã‚’æ¢ç´¢
                    print(f"    ğŸ”§ ä»£æ›¿URLæ¢ç´¢ä¸­...")
                    working_url = self.find_working_feed(domain)
                    
                    if working_url:
                        fixed_urls.append(working_url)
                        print(f"    ğŸ¯ ä¿®æ­£URL: {working_url}")
                    else:
                        print(f"    âŒ ä»£æ›¿URLè¦‹ã¤ã‹ã‚‰ãš")
            
            # URLãŒä¿®æ­£ã•ã‚ŒãŸå ´åˆã¯DBæ›´æ–°
            if not all_working and fixed_urls:
                try:
                    result = supabase.table("sources").update({
                        "urls": fixed_urls
                    }).eq("id", source_id).execute()
                    
                    fixed_count += 1
                    print(f"    âœ… DBæ›´æ–°å®Œäº†: {fixed_urls}")
                except Exception as e:
                    print(f"    âŒ DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(1)
        
        print("\n" + "=" * 60)
        print(f"ğŸ¯ æ¤œè¨¼å®Œäº†")
        print(f"âŒ å•é¡ŒURL: {broken_count} å€‹")
        print(f"ğŸ”§ ä¿®æ­£æˆåŠŸ: {fixed_count} å€‹ã®ã‚½ãƒ¼ã‚¹")
        
        return {
            'total_sources': len(sources),
            'broken_urls': broken_count,
            'fixed_sources': fixed_count
        }
    
    def generate_url_report(self, results: Dict):
        """URLæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        from datetime import datetime
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"url_verification_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# URL Verification & Fix Report\n\n")
            f.write(f"**å®Ÿè¡Œæ—¥æ™‚:** {datetime.now().isoformat()}\n\n")
            
            f.write("## ğŸ“Š æ¤œè¨¼çµæœ\n\n")
            f.write(f"- **æ¤œè¨¼å¯¾è±¡:** {results['total_sources']} sources\n")
            f.write(f"- **å•é¡ŒURL:** {results['broken_urls']} å€‹\n")
            f.write(f"- **ä¿®æ­£æˆåŠŸ:** {results['fixed_sources']} sources\n\n")
            
            f.write("## ğŸ”§ ä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\n\n")
            f.write("### ãƒ‰ãƒ¡ã‚¤ãƒ³ä»£æ›¿\n")
            for original, alternatives in self.domain_alternatives.items():
                f.write(f"- `{original}` â†’ `{', '.join(alternatives)}`\n")
            
            f.write("\n### ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ‘ã‚¹æ¢ç´¢\n")
            for path in self.feed_paths[:8]:  # æœ€åˆã®8å€‹ã®ã¿è¡¨ç¤º
                f.write(f"- `{path}`\n")
            f.write(f"- ãã®ä»– {len(self.feed_paths)-8} ãƒ‘ã‚¿ãƒ¼ãƒ³\n\n")
        
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")

def main():
    verifier = URLVerifier()
    
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    if not os.getenv("SUPABASE_URL") or not os.getenv("SUPABASE_KEY"):
        print("âŒ ã‚¨ãƒ©ãƒ¼: SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    print("ğŸ”§ URL Verification & Fix System")
    print("sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã®urlsã‚’æ¤œè¨¼ã—ã€å•é¡ŒãŒã‚ã‚‹URLã‚’ä¿®æ­£")
    print()
    
    # URLæ¤œè¨¼ãƒ»ä¿®æ­£å®Ÿè¡Œ
    results = verifier.verify_and_fix_all_urls()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    verifier.generate_url_report(results)
    
    print(f"\nâœ… URLæ¤œè¨¼ãƒ»ä¿®æ­£ãŒå®Œäº†ã—ã¾ã—ãŸ!")

if __name__ == "__main__":
    main()