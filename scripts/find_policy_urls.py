#!/usr/bin/env python3
"""
æ—¢å­˜ã®sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã®domainã‚’ä½¿ã£ã¦policy_urlã‚’è‡ªå‹•æ¤œç´¢ãƒ»æ›´æ–°
ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã€åˆ©ç”¨è¦ç´„ã€è‘—ä½œæ¨©ãƒãƒªã‚·ãƒ¼ãªã©ã‚’è‡ªå‹•ç™ºè¦‹
"""
import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import random
from typing import Dict, List, Optional
from supabase import create_client, Client

# Supabaseæ¥ç¶š
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

class PolicyURLFinder:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # å„è¨€èªã§ã®ãƒãƒªã‚·ãƒ¼é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.policy_keywords = {
            'english': [
                'privacy policy', 'privacy', 'terms of service', 'terms of use', 
                'copyright', 'legal', 'disclaimer', 'cookie policy', 'data protection'
            ],
            'japanese': [
                'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼', 'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼', 'å€‹äººæƒ…å ±ä¿è­·æ–¹é‡', 
                'åˆ©ç”¨è¦ç´„', 'è‘—ä½œæ¨©', 'å…è²¬äº‹é …', 'Cookie', 'ã‚¯ãƒƒã‚­ãƒ¼'
            ],
            'chinese': [
                'éšç§æ”¿ç­–', 'éšç§æ¡æ¬¾', 'ä½¿ç”¨æ¡æ¬¾', 'ç‰ˆæƒ', 'å…è´£å£°æ˜', 'æ•°æ®ä¿æŠ¤'
            ],
            'korean': [
                'ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨', 'ê°œì¸ì •ë³´ë³´í˜¸ì •ì±…', 'ì´ìš©ì•½ê´€', 'ì €ì‘ê¶Œ', 'ë©´ì±…ì¡°í•­'
            ],
            'german': [
                'datenschutz', 'datenschutzerklÃ¤rung', 'nutzungsbedingungen', 
                'impressum', 'rechtliches', 'urheberrecht'
            ],
            'french': [
                'politique de confidentialitÃ©', 'mentions lÃ©gales', 'conditions d\'utilisation',
                'droits d\'auteur', 'protection des donnÃ©es'
            ]
        }
        
        # ã‚ˆãã‚ã‚‹ãƒãƒªã‚·ãƒ¼ãƒšãƒ¼ã‚¸ã®ãƒ‘ã‚¹
        self.common_policy_paths = [
            '/privacy', '/privacy-policy', '/privacy.html', '/privacy.php',
            '/terms', '/terms-of-service', '/terms-of-use', '/terms.html',
            '/legal', '/legal-notice', '/disclaimer', '/copyright',
            '/cookie-policy', '/data-protection',
            # æ—¥æœ¬èªã‚µã‚¤ãƒˆç”¨
            '/privacy/', '/policy/', '/legal/', '/terms/',
            '/privacypolicy/', '/privacypolicy.html',
            # ä¸­å›½èªã‚µã‚¤ãƒˆç”¨  
            '/privacy-policy.html', '/terms.html', '/legal.html'
        ]
    
    def get_sources_from_db(self) -> List[Dict]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å…¨sourcesã‚’å–å¾—"""
        try:
            result = supabase.table("sources").select("id, name, domain, country_code, policy_url").execute()
            return result.data
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def try_common_paths(self, domain: str) -> Optional[str]:
        """ã‚ˆãã‚ã‚‹ãƒ‘ã‚¹ã§ãƒãƒªã‚·ãƒ¼ãƒšãƒ¼ã‚¸ã‚’è©¦è¡Œ"""
        base_url = f"https://{domain}"
        
        for path in self.common_policy_paths:
            url = f"{base_url}{path}"
            try:
                response = self.session.head(url, timeout=10, allow_redirects=True)
                if response.status_code == 200:
                    # å®Ÿéš›ã«ãƒãƒªã‚·ãƒ¼é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ç¢ºèª
                    if self.verify_policy_content(url):
                        return url
            except:
                continue
        
        return None
    
    def search_in_page_links(self, domain: str) -> Optional[str]:
        """ã‚µã‚¤ãƒˆã®ãƒ•ãƒƒã‚¿ãƒ¼ãƒ»ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒãƒªã‚·ãƒ¼ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢"""
        try:
            base_url = f"https://{domain}"
            response = self.session.get(base_url, timeout=15)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            target_areas = soup.find_all(['footer', 'header', 'nav', 'aside'])
            if not target_areas:
                # å…¨ä½“ã‹ã‚‰æ¤œç´¢ï¼ˆãƒšãƒ¼ã‚¸ãŒå°ã•ã„å ´åˆï¼‰
                target_areas = [soup]
            
            for area in target_areas:
                links = area.find_all('a', href=True)
                
                for link in links:
                    href = link.get('href')
                    text = link.get_text().lower().strip()
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                    if self.matches_policy_keywords(text):
                        full_url = urljoin(base_url, href)
                        if self.verify_policy_content(full_url):
                            return full_url
        
        except Exception as e:
            print(f"  ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼ ({domain}): {e}")
        
        return None
    
    def matches_policy_keywords(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒãƒãƒªã‚·ãƒ¼é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ãƒãƒƒãƒã™ã‚‹ã‹åˆ¤å®š"""
        text_lower = text.lower()
        
        for lang_keywords in self.policy_keywords.values():
            for keyword in lang_keywords:
                if keyword.lower() in text_lower:
                    return True
        
        return False
    
    def verify_policy_content(self, url: str) -> bool:
        """URLãŒå®Ÿéš›ã«ãƒãƒªã‚·ãƒ¼é–¢é€£ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ç¢ºèª"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code != 200:
                return False
            
            # HTMLã®å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
            if 'text/html' not in response.headers.get('content-type', ''):
                return False
            
            content = response.text.lower()
            
            # ãƒãƒªã‚·ãƒ¼é–¢é€£ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            policy_indicators = [
                'privacy', 'personal information', 'data protection', 'cookie',
                'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼', 'å€‹äººæƒ…å ±', 'åˆ©ç”¨è¦ç´„', 'å…è²¬',
                'éšç§', 'ä¸ªäººä¿¡æ¯', 'ä½¿ç”¨æ¡æ¬¾',
                'ê°œì¸ì •ë³´', 'ì´ìš©ì•½ê´€', 
                'datenschutz', 'donnÃ©es personnelles'
            ]
            
            found_indicators = sum(1 for indicator in policy_indicators if indicator in content)
            
            # æœ€ä½2å€‹ä»¥ä¸Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ãƒãƒªã‚·ãƒ¼ãƒšãƒ¼ã‚¸ã¨åˆ¤å®š
            return found_indicators >= 2
            
        except:
            return False
    
    def find_policy_url(self, domain: str, country_code: str) -> Optional[str]:
        """æŒ‡å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒãƒªã‚·ãƒ¼URLã‚’æ¤œç´¢"""
        print(f"  ğŸ” {domain} ã‚’æ¤œç´¢ä¸­...")
        
        # 1. ã‚ˆãã‚ã‚‹ãƒ‘ã‚¹ã‚’è©¦è¡Œ
        policy_url = self.try_common_paths(domain)
        if policy_url:
            print(f"    âœ“ å…±é€šãƒ‘ã‚¹ã§ç™ºè¦‹: {policy_url}")
            return policy_url
        
        # 2. ãƒšãƒ¼ã‚¸å†…ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
        policy_url = self.search_in_page_links(domain)
        if policy_url:
            print(f"    âœ“ ãƒšãƒ¼ã‚¸å†…ãƒªãƒ³ã‚¯ã§ç™ºè¦‹: {policy_url}")
            return policy_url
        
        print(f"    âŒ ãƒãƒªã‚·ãƒ¼URLæœªç™ºè¦‹")
        return None
    
    def update_policy_url(self, source_id: int, policy_url: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®policy_urlã‚’æ›´æ–°"""
        try:
            result = supabase.table("sources").update({
                "policy_url": policy_url
            }).eq("id", source_id).execute()
            
            return True
        except Exception as e:
            print(f"    âŒ DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def process_all_sources(self, limit: Optional[int] = None):
        """å…¨sourcesã®policy_urlæ¤œç´¢ãƒ»æ›´æ–°ã‚’å®Ÿè¡Œ"""
        sources = self.get_sources_from_db()
        
        if limit:
            sources = sources[:limit]
        
        print(f"ğŸš€ {len(sources)} å€‹ã®ã‚½ãƒ¼ã‚¹ã®ãƒãƒªã‚·ãƒ¼URLæ¤œç´¢é–‹å§‹")
        print("=" * 60)
        
        found_count = 0
        updated_count = 0
        
        for i, source in enumerate(sources, 1):
            source_id = source['id']
            name = source['name']
            domain = source['domain']
            country_code = source.get('country_code', 'Unknown')
            current_policy = source.get('policy_url')
            
            print(f"\n[{i}/{len(sources)}] {name} ({domain})")
            
            # æ—¢ã«policy_urlãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if current_policy:
                print(f"    â„¹ï¸ æ—¢å­˜ã®ãƒãƒªã‚·ãƒ¼URL: {current_policy}")
                continue
            
            # ãƒãƒªã‚·ãƒ¼URLæ¤œç´¢
            policy_url = self.find_policy_url(domain, country_code)
            
            if policy_url:
                found_count += 1
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
                if self.update_policy_url(source_id, policy_url):
                    updated_count += 1
                    print(f"    âœ… DBæ›´æ–°å®Œäº†")
                
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆã‚µã‚¤ãƒˆã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ï¼‰
            time.sleep(random.uniform(2, 4))
        
        print("\n" + "=" * 60)
        print(f"ğŸ¯ å®Œäº†: {found_count} å€‹ã®ãƒãƒªã‚·ãƒ¼URLç™ºè¦‹, {updated_count} å€‹DBæ›´æ–°")
        
        return {
            'processed': len(sources),
            'found': found_count,
            'updated': updated_count
        }
    
    def generate_report(self, results: Dict):
        """çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        from datetime import datetime
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"policy_url_discovery_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Policy URL Discovery Report\n\n")
            f.write(f"**å®Ÿè¡Œæ—¥æ™‚:** {datetime.now().isoformat()}\n\n")
            
            f.write("## ğŸ“Š çµæœã‚µãƒãƒªãƒ¼\n\n")
            f.write(f"- **å‡¦ç†å¯¾è±¡:** {results['processed']} sources\n")
            f.write(f"- **ç™ºè¦‹:** {results['found']} policy URLs\n")
            f.write(f"- **DBæ›´æ–°:** {results['updated']} records\n")
            f.write(f"- **æˆåŠŸç‡:** {results['found']/results['processed']*100:.1f}%\n\n")
            
            f.write("## ğŸ“ æ¤œç´¢å¯¾è±¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n\n")
            for lang, keywords in self.policy_keywords.items():
                f.write(f"### {lang.title()}\n")
                f.write(f"- {', '.join(keywords)}\n\n")
            
            f.write("## ğŸ” æ¤œç´¢ãƒ‘ã‚¹\n\n")
            f.write("### å…±é€šãƒ‘ã‚¹è©¦è¡Œ\n")
            for path in self.common_policy_paths[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
                f.write(f"- `{path}`\n")
            f.write(f"- ãã®ä»– {len(self.common_policy_paths)-10} ãƒ‘ã‚¹\n\n")
            
            f.write("### ãƒšãƒ¼ã‚¸å†…ãƒªãƒ³ã‚¯æ¤œç´¢\n")
            f.write("- ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³å†…ã®ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢\n")
            f.write("- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚° + ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œè¨¼\n\n")
        
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")

def main():
    finder = PolicyURLFinder()
    
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    if not os.getenv("SUPABASE_URL") or not os.getenv("SUPABASE_KEY"):
        print("âŒ ã‚¨ãƒ©ãƒ¼: SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    print("ğŸ” Policy URL Discovery System")
    print("sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã®domainã‹ã‚‰policy_urlã‚’è‡ªå‹•æ¤œç´¢ãƒ»æ›´æ–°")
    print()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆæœ€åˆã®5å€‹ã®ã¿ï¼‰
    test_mode = os.getenv("TEST_MODE", "false").lower() == "true"
    limit = 5 if test_mode else None
    
    if test_mode:
        print("ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®5å€‹ã®ã‚½ãƒ¼ã‚¹ã®ã¿å‡¦ç†")
    
    # ãƒãƒªã‚·ãƒ¼URLæ¤œç´¢å®Ÿè¡Œ
    results = finder.process_all_sources(limit=limit)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    finder.generate_report(results)
    
    print(f"\nâœ… Policy URL discovery completed!")

if __name__ == "__main__":
    main()