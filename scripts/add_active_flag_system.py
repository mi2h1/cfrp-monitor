#!/usr/bin/env python3
"""
sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã«activeãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ã—ã¦å–å¾—åˆ¶å¾¡
- å–å¾—å›°é›£ãªã‚½ãƒ¼ã‚¹ã¯ä¸€æ™‚åœæ­¢
- æƒ…å ±æºã¨ã—ã¦ã¯ä¿æŒ
- å®šæ™‚ã‚¿ã‚¹ã‚¯ã§ã¯ active=true ã®ã¿å‡¦ç†
"""
import os
import requests
from urllib.parse import urlparse
import time
from typing import Dict, List, Optional
from supabase import create_client, Client

# Supabaseæ¥ç¶š
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

class SourceActivationManager:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def add_active_column(self):
        """sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã«activeã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆSQLå®Ÿè¡ŒãŒå¿…è¦ï¼‰"""
        sql_commands = [
            "-- sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã«activeãƒ•ãƒ©ã‚°è¿½åŠ ",
            "ALTER TABLE sources ADD COLUMN IF NOT EXISTS active BOOLEAN DEFAULT true;",
            "",
            "-- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å…¨ã¦ã®ã‚½ãƒ¼ã‚¹ã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«è¨­å®š",
            "UPDATE sources SET active = true WHERE active IS NULL;",
            "",
            "-- activeã‚«ãƒ©ãƒ ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰",
            "CREATE INDEX IF NOT EXISTS idx_sources_active ON sources(active);",
            "",
            "-- ç¢ºèªç”¨ã‚¯ã‚¨ãƒª",
            "-- SELECT name, domain, active FROM sources ORDER BY active DESC, name;"
        ]
        
        with open('add_active_column.sql', 'w', encoding='utf-8') as f:
            f.write('\n'.join(sql_commands))
        
        print("ğŸ“„ SQLç”Ÿæˆå®Œäº†: add_active_column.sql")
        print("ã“ã®SQLã‚’Supabaseã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
    def test_source_accessibility(self, source: Dict) -> Dict:
        """ã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = {
            'source_id': source['id'],
            'name': source['name'],
            'domain': source['domain'],
            'urls': source.get('urls', []),
            'accessible': False,
            'issues': [],
            'recommended_active': True
        }
        
        if not result['urls']:
            result['issues'].append('URLæœªè¨­å®š')
            result['recommended_active'] = False
            return result
        
        for url in result['urls']:
            print(f"  ãƒ†ã‚¹ãƒˆä¸­: {url}")
            
            try:
                # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§çŠ¶æ…‹ç¢ºèª
                response = self.session.head(url, timeout=10, allow_redirects=True)
                
                if response.status_code == 200:
                    print(f"    âœ… OK ({response.status_code})")
                    result['accessible'] = True
                    break
                elif response.status_code == 404:
                    print(f"    âŒ 404 Not Found")
                    result['issues'].append('404 Not Found')
                elif response.status_code in [403, 429]:
                    print(f"    âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ ({response.status_code})")
                    result['issues'].append(f'ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ ({response.status_code})')
                else:
                    print(f"    âš ï¸ å¿œç­”ç•°å¸¸ ({response.status_code})")
                    result['issues'].append(f'å¿œç­”ç•°å¸¸ ({response.status_code})')
                    
            except requests.exceptions.SSLError:
                print(f"    âŒ SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼")
                result['issues'].append('SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼')
            except requests.exceptions.ConnectionError:
                print(f"    âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼")
                result['issues'].append('æ¥ç¶šã‚¨ãƒ©ãƒ¼')
            except requests.exceptions.Timeout:
                print(f"    âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                result['issues'].append('ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ')
            except Exception as e:
                print(f"    âŒ ãã®ä»–ã‚¨ãƒ©ãƒ¼: {e}")
                result['issues'].append(f'ãã®ä»–ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}')
        
        # ã‚¢ã‚¯ã‚»ã‚¹å›°é›£ãªå ´åˆã¯éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚’æ¨å¥¨
        if not result['accessible'] and result['issues']:
            if any(issue in ['404 Not Found', 'SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼', 'æ¥ç¶šã‚¨ãƒ©ãƒ¼'] for issue in result['issues']):
                result['recommended_active'] = False
        
        return result
    
    def get_all_sources(self) -> List[Dict]:
        """å…¨sourcesã‚’å–å¾—"""
        try:
            result = supabase.table("sources").select("id, name, domain, urls, active").execute()
            return result.data
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def update_source_active_status(self, source_id: int, active: bool, reason: str = "") -> bool:
        """ã‚½ãƒ¼ã‚¹ã®activeã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°"""
        try:
            data = {"active": active}
            if reason:
                data["notes"] = f"Auto-disabled: {reason}"
            
            result = supabase.table("sources").update(data).eq("id", source_id).execute()
            return True
        except Exception as e:
            print(f"    âŒ DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def analyze_all_sources(self):
        """å…¨ã‚½ãƒ¼ã‚¹ã®åˆ†æã¨activeã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°"""
        sources = self.get_all_sources()
        
        print(f"ğŸ” {len(sources)} å€‹ã®ã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½æ€§ã‚’åˆ†æ")
        print("=" * 60)
        
        active_count = 0
        inactive_count = 0
        updated_count = 0
        
        results = []
        
        for i, source in enumerate(sources, 1):
            print(f"\n[{i}/{len(sources)}] {source['name']} ({source['domain']})")
            
            # ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ
            test_result = self.test_source_accessibility(source)
            results.append(test_result)
            
            current_active = source.get('active', True)
            recommended_active = test_result['recommended_active']
            
            if current_active != recommended_active:
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
                reason = ', '.join(test_result['issues'][:2])  # æœ€åˆã®2ã¤ã®å•é¡Œ
                if self.update_source_active_status(source['id'], recommended_active, reason):
                    updated_count += 1
                    status_change = "âœ… ã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–" if recommended_active else "â¸ï¸ éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–"
                    print(f"    {status_change}: {reason}")
            
            if recommended_active:
                active_count += 1
            else:
                inactive_count += 1
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(1)
        
        print("\n" + "=" * 60)
        print(f"ğŸ¯ åˆ†æå®Œäº†")
        print(f"âœ… ã‚¢ã‚¯ãƒ†ã‚£ãƒ–: {active_count} å€‹")
        print(f"â¸ï¸ éã‚¢ã‚¯ãƒ†ã‚£ãƒ–: {inactive_count} å€‹")
        print(f"ğŸ”„ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°: {updated_count} å€‹")
        
        return results
    
    def generate_status_report(self, results: List[Dict]):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        from datetime import datetime
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"source_activation_report_{timestamp}.md"
        
        active_sources = [r for r in results if r['recommended_active']]
        inactive_sources = [r for r in results if not r['recommended_active']]
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Source Activation Status Report\n\n")
            f.write(f"**å®Ÿè¡Œæ—¥æ™‚:** {datetime.now().isoformat()}\n\n")
            
            f.write("## ğŸ“Š æ¦‚è¦\n\n")
            f.write(f"- **ç·ã‚½ãƒ¼ã‚¹æ•°:** {len(results)}\n")
            f.write(f"- **ã‚¢ã‚¯ãƒ†ã‚£ãƒ–:** {len(active_sources)} ({len(active_sources)/len(results)*100:.1f}%)\n")
            f.write(f"- **éã‚¢ã‚¯ãƒ†ã‚£ãƒ–:** {len(inactive_sources)} ({len(inactive_sources)/len(results)*100:.1f}%)\n\n")
            
            if active_sources:
                f.write("## âœ… ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚½ãƒ¼ã‚¹\n\n")
                for source in active_sources:
                    f.write(f"### {source['name']}\n")
                    f.write(f"- **ãƒ‰ãƒ¡ã‚¤ãƒ³:** {source['domain']}\n")
                    f.write(f"- **URLæ•°:** {len(source['urls'])}\n")
                    f.write(f"- **ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** æ­£å¸¸\n\n")
            
            if inactive_sources:
                f.write("## â¸ï¸ éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚½ãƒ¼ã‚¹\n\n")
                for source in inactive_sources:
                    f.write(f"### {source['name']}\n")
                    f.write(f"- **ãƒ‰ãƒ¡ã‚¤ãƒ³:** {source['domain']}\n")
                    f.write(f"- **å•é¡Œ:** {', '.join(source['issues'])}\n")
                    f.write(f"- **æ¨å¥¨:** ä¸€æ™‚åœæ­¢\n\n")
        
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")

def update_crawl_script():
    """crawl.pyã«activeãƒ•ã‚£ãƒ«ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ"""
    
    crawl_update_code = '''
# crawl.py ã®ä¿®æ­£ï¼ˆactiveãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ ï¼‰

# å¤‰æ›´å‰:
# sources_result = supabase.table("sources").select("*").eq("acquisition_mode", "auto").execute()

# å¤‰æ›´å¾Œ:
sources_result = supabase.table("sources").select("*").eq("acquisition_mode", "auto").eq("active", True).execute()

print(f"è‡ªå‹•åé›†å¯¾è±¡ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã®ã¿ï¼‰: {len(sources.data)} ä»¶")
'''
    
    with open('crawl_active_filter.py', 'w', encoding='utf-8') as f:
        f.write(crawl_update_code)
    
    print("ğŸ“„ crawl.pyä¿®æ­£ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ: crawl_active_filter.py")

def main():
    manager = SourceActivationManager()
    
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    if not os.getenv("SUPABASE_URL") or not os.getenv("SUPABASE_KEY"):
        print("âŒ ã‚¨ãƒ©ãƒ¼: SUPABASE_URL ã¨ SUPABASE_KEY ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    print("ğŸ›ï¸ Source Activation Management System")
    print("ã‚¢ã‚¯ã‚»ã‚¹å›°é›£ãªã‚½ãƒ¼ã‚¹ã‚’è‡ªå‹•çš„ã«éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–")
    print()
    
    # Step 1: activeã‚«ãƒ©ãƒ è¿½åŠ SQLç”Ÿæˆ
    print("ğŸ“‹ Step 1: activeã‚«ãƒ©ãƒ è¿½åŠ ")
    manager.add_active_column()
    
    print("\nâ³ add_active_column.sql ã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰ç¶šè¡Œã—ã¦ãã ã•ã„...")
    input("SQLã‚’å®Ÿè¡Œã—ãŸã‚‰ Enter ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
    
    # Step 2: å…¨ã‚½ãƒ¼ã‚¹ã®åˆ†æ
    print("\nğŸ“‹ Step 2: ã‚½ãƒ¼ã‚¹åˆ†æå®Ÿè¡Œ")
    results = manager.analyze_all_sources()
    
    # Step 3: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    manager.generate_status_report(results)
    
    # Step 4: crawl.pyä¿®æ­£ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
    update_crawl_script()
    
    print("\nğŸ¯ å®Œäº†! æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. crawl_active_filter.py ã®ã‚³ãƒ¼ãƒ‰ã‚’ crawl.py ã«é©ç”¨")
    print("2. éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚½ãƒ¼ã‚¹ã¯å®šæ™‚ã‚¿ã‚¹ã‚¯ã‹ã‚‰é™¤å¤–ã•ã‚Œã‚‹")
    print("3. å°†æ¥çš„ã«ä¿®å¾©å¯èƒ½ã«ãªã£ãŸã‚‰æ‰‹å‹•ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–")

if __name__ == "__main__":
    main()