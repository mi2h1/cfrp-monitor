#!/usr/bin/env python3
import os, json, datetime, pathlib, requests, trafilatura
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from readability import Document
from supabase import create_client, Client
from dateutil import parser as dtparser
from fetcher import fetch_and_parse, slug, DEFAULT_CFG

# â”€â”€ Supabase â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
supabase: Client = create_client(os.getenv("SUPABASE_URL"),
                                 os.getenv("SUPABASE_KEY"))

# â”€â”€ ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
today   = datetime.date.today().isoformat()
RAW_DIR = pathlib.Path("raw") / today
RAW_DIR.mkdir(parents=True, exist_ok=True)

def save_raw(name: str, data):
    (RAW_DIR / f"{name}.json").write_text(
        json.dumps(data, ensure_ascii=False, indent=2, default=str),
        encoding="utf-8"
    )

def safe_date(txt):
    try:
        return dtparser.parse(txt).date().isoformat() if txt else None
    except Exception:
        return None

def upsert(row: dict):
    res = supabase.table("items").upsert(row, on_conflict="url").execute()
    err = getattr(res, "error", None) or (res.get("error") if isinstance(res, dict) else None)
    print("UPSERT", "ERROR:" if err else "OK:", err or row["url"])


# â”€â”€ HTML æœ¬æ–‡æŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
UA_HEADERS = {"User-Agent": DEFAULT_CFG["ua"]}
retry_cfg  = Retry(total=3, backoff_factor=1,
                   status_forcelist=[429, 502, 503, 504])
session = requests.Session()
session.headers.update(UA_HEADERS)
session.mount("https://", HTTPAdapter(max_retries=retry_cfg))
session.mount("http://",  HTTPAdapter(max_retries=retry_cfg))

def extract_html_body(html: str) -> str | None:
    """æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆTrafilatura â†’ Readability ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    text = trafilatura.extract(html, include_comments=False, include_tables=False)
    if text and len(text.split()) > 50:
        return text
    # fallback
    try:
        cleaned = trafilatura.extract(Document(html).summary())
        return cleaned
    except Exception:
        return None

def fetch_article_body(url: str) -> str | None:
    try:
        r = session.get(url, timeout=25)
        r.raise_for_status()
        return extract_html_body(r.text)
    except Exception as e:
        print("âš ï¸ body fetch failed:", url, "->", e)
        return None


# â”€â”€ ã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ï¼ˆSupabaseã‹ã‚‰ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sources_result = supabase.table("sources").select("*").eq("acquisition_mode", "auto").execute()
sources = sources_result.data

print(f"ğŸ“Š è‡ªå‹•åé›†å¯¾è±¡: {len(sources)} ä»¶")

# â”€â”€ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for src in sources:
    # sourcesãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ ã‹ã‚‰seed_sources.ymläº’æ›ã®è¨­å®šã‚’æ§‹ç¯‰
    cfg = {
        **DEFAULT_CFG,
        "ua": src.get("ua") or DEFAULT_CFG["ua"],
        "http_fallback": src.get("http_fallback", False),
        "retry": src.get("retry_count", 3),
        "backoff": src.get("backoff_factor", 1.0),
        "parser": src.get("parser", "rss")
    }
    
    urls = src.get("urls") or []
    if not urls:
        print(f"âš ï¸  URLæœªè¨­å®š: {src.get('name', src.get('domain'))}")
        continue

    for feed_url in urls:
        entries = fetch_and_parse(feed_url, cfg)
        save_raw(f'{src["name"]}-{slug(feed_url)}', entries)

        for e in entries:
            # --- 1) ãƒ•ã‚£ãƒ¼ãƒ‰ã«æœ¬æ–‡ãŒã‚ã‚‹ã‹ç¢ºèª --------------------------
            body = None
            if "content" in e and e.content:
                body = e.content[0].value
            elif e.get("summary_detail", {}).get("type") == "text/html":
                body = e.get("summary")

            # --- 2) ç„¡ã‘ã‚Œã°ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ« ----------------------------
            if not body:
                link = e.get("link") or e.get("id")
                body = fetch_article_body(link)

            upsert({
                "src_type": src["category"],
                "source_id": src["id"],  # å¤–éƒ¨ã‚­ãƒ¼è¿½åŠ 
                "title"   : e.get("title"),
                "url"     : e.get("link") or e.get("id"),
                "pub_date": safe_date(e.get("published") or e.get("updated")),
                "body"    : body,
            })

print("âœ… crawl finished")
