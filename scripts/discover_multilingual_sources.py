#!/usr/bin/env python3
"""
å¤šè¨€èªå¯¾å¿œã®è¤‡åˆææ–™æƒ…å ±æºç™ºè¦‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¥æœ¬èªã€ãƒ‰ã‚¤ãƒ„èªã€ãƒ•ãƒ©ãƒ³ã‚¹èªã€ä¸­å›½èªãªã©ã®æƒ…å ±æºã‚’æ¤œç´¢
"""
import os
import json
import time
import requests
import feedparser
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional
from supabase import create_client, Client

# Supabaseæ¥ç¶š
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

class MultilingualSourceDiscoverer:
    def __init__(self):
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cx = os.getenv("GOOGLE_CUSTOM_SEARCH_ENGINE_ID")
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; MultilingualCompositeDiscoverer/1.0)'
        })
        
        # è¨€èªåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©
        self.multilingual_keywords = {
            'japanese': {
                'basic': ['è¤‡åˆææ–™', 'è¤‡åˆç´ æ', 'ã‚³ãƒ³ãƒã‚¸ãƒƒãƒˆ', 'ç‚­ç´ ç¹Šç¶­', 'ã‚«ãƒ¼ãƒœãƒ³ãƒ•ã‚¡ã‚¤ãƒãƒ¼', 'ã‚¬ãƒ©ã‚¹ç¹Šç¶­', 'CFRP', 'GFRP'],
                'technical': ['ãƒ—ãƒªãƒ—ãƒ¬ã‚°', 'ã‚ªãƒ¼ãƒˆã‚¯ãƒ¬ãƒ¼ãƒ–', 'æ¨¹è„‚ç§»æ³¨æˆå½¢', 'RTM', 'å¼•æŠœæˆå½¢', 'ãƒ•ã‚£ãƒ©ãƒ¡ãƒ³ãƒˆãƒ¯ã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°'],
                'applications': ['èˆªç©ºå®‡å®™', 'è‡ªå‹•è»Š', 'é¢¨åŠ›ç™ºé›»', 'èˆ¹èˆ¶', 'å»ºè¨­'],
                'organizations': ['å­¦ä¼š', 'å”ä¼š', 'ç ”ç©¶æ‰€', 'æŠ€è¡“ã‚»ãƒ³ã‚¿ãƒ¼'],
                'media': ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'é›‘èªŒ', 'æ¥­ç•ŒèªŒ', 'æŠ€è¡“èªŒ', 'å·¥æ¥­æ–°è']
            },
            'german': {
                'basic': ['Verbundwerkstoff', 'Komposit', 'Faserverbund', 'Kohlenstofffaser', 'Glasfaser', 'CFK', 'GFK'],
                'technical': ['Prepreg', 'Autoklav', 'RTM', 'Pultrudieren', 'Wickeltechnik'],
                'applications': ['Luftfahrt', 'Automobil', 'Windenergie', 'Schiffbau', 'Bauwesen'],
                'organizations': ['Forschung', 'Institut', 'Verein', 'Gesellschaft'],
                'media': ['Nachrichten', 'Zeitschrift', 'Fachmagazin', 'Industrie']
            },
            'french': {
                'basic': ['matÃ©riaux composites', 'composite', 'fibre de carbone', 'fibre de verre', 'CFRP', 'GFRP'],
                'technical': ['prÃ©imprÃ©gnÃ©', 'autoclave', 'RTM', 'pultrusion', 'bobinage'],
                'applications': ['aÃ©ronautique', 'automobile', 'Ã©olienne', 'naval', 'construction'],
                'organizations': ['recherche', 'institut', 'association', 'sociÃ©tÃ©'],
                'media': ['actualitÃ©s', 'magazine', 'revue', 'industrie']
            },
            'chinese': {
                'basic': ['å¤åˆææ–™', 'ç¢³çº¤ç»´', 'ç»ç’ƒçº¤ç»´', 'å¤åˆæ', 'ç¢³çº¤', 'çº¤ç»´å¢å¼º'],
                'technical': ['é¢„æµ¸æ–™', 'é«˜å‹é‡œ', 'æ ‘è„‚ä¼ é€’æ¨¡å¡‘', 'RTM', 'æ‹‰æŒ¤æˆå‹', 'ç¼ ç»•æˆå‹'],
                'applications': ['èˆªç©ºèˆªå¤©', 'æ±½è½¦', 'é£åŠ›å‘ç”µ', 'èˆ¹èˆ¶', 'å»ºç­‘'],
                'organizations': ['å­¦ä¼š', 'åä¼š', 'ç ”ç©¶æ‰€', 'æŠ€æœ¯ä¸­å¿ƒ'],
                'media': ['æ–°é—»', 'æ‚å¿—', 'æœŸåˆŠ', 'å·¥ä¸š']
            },
            'korean': {
                'basic': ['ë³µí•©ì¬ë£Œ', 'íƒ„ì†Œì„¬ìœ ', 'ìœ ë¦¬ì„¬ìœ ', 'ë³µí•©ì†Œì¬', 'CFRP', 'GFRP'],
                'technical': ['í”„ë¦¬í”„ë ˆê·¸', 'ì˜¤í† í´ë ˆì´ë¸Œ', 'RTM', 'ì¸ë°œì„±í˜•', 'í•„ë¼ë©˜íŠ¸ì™€ì¸ë”©'],
                'applications': ['í•­ê³µìš°ì£¼', 'ìë™ì°¨', 'í’ë ¥ë°œì „', 'ì¡°ì„ ', 'ê±´ì„¤'],
                'organizations': ['í•™íšŒ', 'í˜‘íšŒ', 'ì—°êµ¬ì†Œ', 'ê¸°ìˆ ì„¼í„°'],
                'media': ['ë‰´ìŠ¤', 'ì¡ì§€', 'ê¸°ìˆ ì§€', 'ì‚°ì—…']
            }
        }
        
        # è¨€èªåˆ¥ãƒ‰ãƒ¡ã‚¤ãƒ³å‚¾å‘
        self.language_domains = {
            'japanese': ['.jp', '.co.jp', '.or.jp', '.ac.jp'],
            'german': ['.de', '.at', '.ch'],
            'french': ['.fr', '.be', '.ch'],
            'chinese': ['.cn', '.com.cn', '.edu.cn'],
            'korean': ['.kr', '.co.kr'],
            'italian': ['.it'],
            'spanish': ['.es'],
            'russian': ['.ru']
        }
        
    def search_multilingual(self, language: str, query_type: str = 'basic') -> List[Dict]:
        """æŒ‡å®šè¨€èªã§ã®è¤‡åˆææ–™æƒ…å ±æºã‚’æ¤œç´¢"""
        if language not in self.multilingual_keywords:
            print(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„è¨€èª: {language}")
            return []
            
        keywords = self.multilingual_keywords[language].get(query_type, [])
        if not keywords:
            return []
            
        # æ¤œç´¢ã‚¯ã‚¨ãƒªæ§‹ç¯‰
        queries = []
        
        if language == 'japanese':
            queries = [
                f'"{keywords[0]}" OR "{keywords[1]}" ãƒ‹ãƒ¥ãƒ¼ã‚¹ RSS site:.jp',
                f'"{keywords[2]}" OR "{keywords[3]}" å­¦ä¼š RSS',
                f'"ç‚­ç´ ç¹Šç¶­" OR "ã‚«ãƒ¼ãƒœãƒ³ãƒ•ã‚¡ã‚¤ãƒãƒ¼" æ¥­ç•Œ RSS',
                f'è¤‡åˆææ–™ å”ä¼š OR å­¦ä¼š RSS'
            ]
        elif language == 'german':
            queries = [
                f'"{keywords[0]}" OR "{keywords[1]}" Nachrichten RSS site:.de',
                f'"Faserverbund" Forschung RSS',
                f'"Kohlenstofffaser" Industrie RSS'
            ]
        elif language == 'french':
            queries = [
                f'"{keywords[0]}" actualitÃ©s RSS site:.fr',
                f'"fibre de carbone" industrie RSS',
                f'composite aÃ©ronautique RSS'
            ]
        elif language == 'chinese':
            queries = [
                f'"{keywords[0]}" æ–°é—» RSS site:.cn',
                f'"ç¢³çº¤ç»´" è¡Œä¸š RSS',
                f'å¤åˆææ–™ å­¦ä¼š RSS'
            ]
        elif language == 'korean':
            queries = [
                f'"{keywords[0]}" ë‰´ìŠ¤ RSS site:.kr',
                f'"íƒ„ì†Œì„¬ìœ " ì‚°ì—… RSS',
                f'ë³µí•©ì¬ë£Œ í•™íšŒ RSS'
            ]
        
        all_results = []
        for query in queries:
            print(f"\næ¤œç´¢ä¸­ ({language}): {query}")
            results = self.search_google(query, num_results=5)
            all_results.extend(results)
            time.sleep(2)  # APIåˆ¶é™å¯¾ç­–
            
        return all_results
    
    def search_google(self, query: str, num_results: int = 5) -> List[Dict]:
        """Googleæ¤œç´¢å®Ÿè¡Œ"""
        if not self.google_api_key or not self.google_cx:
            return []
            
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            'key': self.google_api_key,
            'cx': self.google_cx,
            'q': query,
            'num': num_results
        }
        
        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            return data.get('items', [])
        except Exception as e:
            print(f"Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def detect_language_from_domain(self, url: str) -> str:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰è¨€èªã‚’æ¨å®š"""
        domain = urlparse(url).netloc.lower()
        
        for language, domains in self.language_domains.items():
            if any(domain.endswith(d) for d in domains):
                return language
                
        return 'english'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def evaluate_multilingual_relevance(self, url: str, title: str, snippet: str, language: str) -> float:
        """è¨€èªåˆ¥ã®é–¢é€£åº¦è©•ä¾¡"""
        score = 0.0
        text = f"{title} {snippet} {url}".lower()
        
        if language not in self.multilingual_keywords:
            return 0.0
            
        # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        for category, keywords in self.multilingual_keywords[language].items():
            category_score = 0.2 if category == 'basic' else 0.1
            for keyword in keywords:
                if keyword.lower() in text:
                    score += category_score
                    
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¨€èªä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
        detected_lang = self.detect_language_from_domain(url)
        if detected_lang == language:
            score += 0.3
            
        return min(score, 1.0)
    
    def find_rss_feeds(self, url: str) -> List[str]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰æ¤œå‡ºï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰"""
        feeds = []
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # RSSãƒªãƒ³ã‚¯æ¤œå‡º
            rss_links = soup.find_all('link', type=['application/rss+xml', 'application/atom+xml'])
            for link in rss_links:
                href = link.get('href')
                if href:
                    feed_url = urljoin(url, href)
                    feeds.append(feed_url)
            
            # è¨€èªåˆ¥ã®ä¸€èˆ¬çš„ãªRSSãƒ‘ã‚¹
            common_paths = ['/rss', '/feed', '/rss.xml', '/feed.xml', '/atom.xml']
            
            # æ—¥æœ¬èªã‚µã‚¤ãƒˆç‰¹æœ‰ã®ãƒ‘ã‚¹
            if self.detect_language_from_domain(url) == 'japanese':
                common_paths.extend(['/news/rss', '/info/rss', '/topics/rss'])
            
            for path in common_paths:
                feed_url = urljoin(url, path)
                if self.validate_feed(feed_url):
                    feeds.append(feed_url)
                    
        except Exception as e:
            print(f"RSSæ¤œå‡ºã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            
        return list(set(feeds))
    
    def validate_feed(self, feed_url: str) -> bool:
        """ãƒ•ã‚£ãƒ¼ãƒ‰æœ‰åŠ¹æ€§ç¢ºèª"""
        try:
            response = self.session.get(feed_url, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                return len(feed.entries) > 0
        except:
            pass
        return False
    
    def discover_multilingual_sources(self):
        """å¤šè¨€èªã®è¤‡åˆææ–™æƒ…å ±æºã‚’ç™ºè¦‹"""
        target_languages = ['japanese', 'german', 'chinese', 'korean']  # å„ªå…ˆè¨€èª
        
        all_discovered = {}
        
        for language in target_languages:
            print(f"\n{'='*50}")
            print(f"ğŸŒ {language.upper()} æƒ…å ±æºã®æ¤œç´¢é–‹å§‹")
            print(f"{'='*50}")
            
            # åŸºæœ¬æ¤œç´¢
            results = self.search_multilingual(language, 'basic')
            
            language_sources = []
            for result in results:
                site_url = result.get('link', '')
                title = result.get('title', '')
                snippet = result.get('snippet', '')
                
                # é–¢é€£åº¦è©•ä¾¡
                relevance = self.evaluate_multilingual_relevance(site_url, title, snippet, language)
                if relevance < 0.3:
                    continue
                    
                print(f"\nèª¿æŸ»ä¸­: {title}")
                print(f"URL: {site_url}")
                print(f"é–¢é€£åº¦: {relevance:.2f}")
                
                # RSSãƒ•ã‚£ãƒ¼ãƒ‰æ¤œå‡º
                feeds = self.find_rss_feeds(site_url)
                
                for feed_url in feeds:
                    source = {
                        'name': title or urlparse(site_url).netloc,
                        'urls': [feed_url],
                        'site_url': site_url,
                        'language': language,
                        'relevance_score': relevance,
                        'discovered_at': datetime.now().isoformat()
                    }
                    language_sources.append(source)
                    print(f"  â†’ RSSãƒ•ã‚£ãƒ¼ãƒ‰ç™ºè¦‹: {feed_url}")
            
            all_discovered[language] = language_sources
            print(f"\n{language}: {len(language_sources)} ä»¶ã®æƒ…å ±æºã‚’ç™ºè¦‹")
        
        return all_discovered
    
    def save_multilingual_candidates(self, sources_by_language: Dict[str, List[Dict]]):
        """å¤šè¨€èªå€™è£œã®ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for language, sources in sources_by_language.items():
            if not sources:
                continue
                
            filename = f"multilingual_sources_{language}_{timestamp}.json"
            filepath = os.path.join(os.path.dirname(__file__), '..', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(sources, f, ensure_ascii=False, indent=2)
                
            print(f"\n{language}: {len(sources)} ä»¶ã®å€™è£œã‚’ä¿å­˜ â†’ {filename}")
        
        # çµ±è¨ˆæƒ…å ±
        total_sources = sum(len(sources) for sources in sources_by_language.values())
        print(f"\nç·è¨ˆ: {total_sources} ä»¶ã®å¤šè¨€èªæƒ…å ±æºå€™è£œã‚’ç™ºè¦‹")
        
        print("\nè¨€èªåˆ¥å†…è¨³:")
        for language, sources in sources_by_language.items():
            if sources:
                print(f"  {language}: {len(sources)} ä»¶")

def main():
    discoverer = MultilingualSourceDiscoverer()
    
    if not discoverer.google_api_key:
        print("è­¦å‘Š: Google APIèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("å¤šè¨€èªæ¤œç´¢ã«ã¯Google Custom Search APIãŒå¿…è¦ã§ã™")
        return
    
    print("å¤šè¨€èªè¤‡åˆææ–™æƒ…å ±æºã®æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™...")
    print("å¯¾è±¡è¨€èª: æ—¥æœ¬èªã€ãƒ‰ã‚¤ãƒ„èªã€ä¸­å›½èªã€éŸ“å›½èª")
    
    sources = discoverer.discover_multilingual_sources()
    discoverer.save_multilingual_candidates(sources)

if __name__ == "__main__":
    main()