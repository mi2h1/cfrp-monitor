#!/usr/bin/env python3
"""
å¤šè¨€èªå¯¾å¿œã®è¤‡åˆææ–™æƒ…å ±æºç™ºè¦‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¥æœ¬èªã€ãƒ‰ã‚¤ãƒ„èªã€ãƒ•ãƒ©ãƒ³ã‚¹èªã€ä¸­å›½èªãªã©ã®æƒ…å ±æºã‚’æ¤œç´¢
"""
import os
import json
import time
import requests
import feedparser
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional
from supabase import create_client, Client

# Supabaseæ¥ç¶š
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

class MultilingualSourceDiscoverer:
    def __init__(self):
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cx = os.getenv("GOOGLE_CUSTOM_SEARCH_ENGINE_ID")
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; MultilingualCompositeDiscoverer/1.0)'
        })
        
        # è¨€èªåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©
        self.multilingual_keywords = {
            'japanese': {
                'basic': ['è¤‡åˆææ–™', 'è¤‡åˆç´ æ', 'ã‚³ãƒ³ãƒã‚¸ãƒƒãƒˆ', 'ç‚­ç´ ç¹Šç¶­', 'ã‚«ãƒ¼ãƒœãƒ³ãƒ•ã‚¡ã‚¤ãƒãƒ¼', 'ã‚¬ãƒ©ã‚¹ç¹Šç¶­', 'CFRP', 'GFRP'],
                'technical': ['ãƒ—ãƒªãƒ—ãƒ¬ã‚°', 'ã‚ªãƒ¼ãƒˆã‚¯ãƒ¬ãƒ¼ãƒ–', 'æ¨¹è„‚ç§»æ³¨æˆå½¢', 'RTM', 'å¼•æŠœæˆå½¢', 'ãƒ•ã‚£ãƒ©ãƒ¡ãƒ³ãƒˆãƒ¯ã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°'],
                'applications': ['èˆªç©ºå®‡å®™', 'è‡ªå‹•è»Š', 'é¢¨åŠ›ç™ºé›»', 'èˆ¹èˆ¶', 'å»ºè¨­'],
                'organizations': ['å­¦ä¼š', 'å”ä¼š', 'ç ”ç©¶æ‰€', 'æŠ€è¡“ã‚»ãƒ³ã‚¿ãƒ¼'],
                'media': ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'é›‘èªŒ', 'æ¥­ç•ŒèªŒ', 'æŠ€è¡“èªŒ', 'å·¥æ¥­æ–°è']
            },
            'german': {
                'basic': ['Verbundwerkstoff', 'Komposit', 'Faserverbund', 'Kohlenstofffaser', 'Glasfaser', 'CFK', 'GFK'],
                'technical': ['Prepreg', 'Autoklav', 'RTM', 'Pultrudieren', 'Wickeltechnik'],
                'applications': ['Luftfahrt', 'Automobil', 'Windenergie', 'Schiffbau', 'Bauwesen'],
                'organizations': ['Forschung', 'Institut', 'Verein', 'Gesellschaft'],
                'media': ['Nachrichten', 'Zeitschrift', 'Fachmagazin', 'Industrie']
            },
            'french': {
                'basic': ['matÃ©riaux composites', 'composite', 'fibre de carbone', 'fibre de verre', 'CFRP', 'GFRP'],
                'technical': ['prÃ©imprÃ©gnÃ©', 'autoclave', 'RTM', 'pultrusion', 'bobinage'],
                'applications': ['aÃ©ronautique', 'automobile', 'Ã©olienne', 'naval', 'construction'],
                'organizations': ['recherche', 'institut', 'association', 'sociÃ©tÃ©'],
                'media': ['actualitÃ©s', 'magazine', 'revue', 'industrie']
            },
            'chinese': {
                'basic': ['å¤åˆææ–™', 'ç¢³çº¤ç»´', 'ç»ç’ƒçº¤ç»´', 'å¤åˆæ', 'ç¢³çº¤', 'çº¤ç»´å¢å¼º'],
                'technical': ['é¢„æµ¸æ–™', 'é«˜å‹é‡œ', 'æ ‘è„‚ä¼ é€’æ¨¡å¡‘', 'RTM', 'æ‹‰æŒ¤æˆå‹', 'ç¼ ç»•æˆå‹'],
                'applications': ['èˆªç©ºèˆªå¤©', 'æ±½è½¦', 'é£åŠ›å‘ç”µ', 'èˆ¹èˆ¶', 'å»ºç­‘'],
                'organizations': ['å­¦ä¼š', 'åä¼š', 'ç ”ç©¶æ‰€', 'æŠ€æœ¯ä¸­å¿ƒ'],
                'media': ['æ–°é—»', 'æ‚å¿—', 'æœŸåˆŠ', 'å·¥ä¸š']
            },
            'korean': {
                'basic': ['ë³µí•©ì¬ë£Œ', 'íƒ„ì†Œì„¬ìœ ', 'ìœ ë¦¬ì„¬ìœ ', 'ë³µí•©ì†Œì¬', 'CFRP', 'GFRP'],
                'technical': ['í”„ë¦¬í”„ë ˆê·¸', 'ì˜¤í† í´ë ˆì´ë¸Œ', 'RTM', 'ì¸ë°œì„±í˜•', 'í•„ë¼ë©˜íŠ¸ì™€ì¸ë”©'],
                'applications': ['í•­ê³µìš°ì£¼', 'ìë™ì°¨', 'í’ë ¥ë°œì „', 'ì¡°ì„ ', 'ê±´ì„¤'],
                'organizations': ['í•™íšŒ', 'í˜‘íšŒ', 'ì—°êµ¬ì†Œ', 'ê¸°ìˆ ì„¼í„°'],
                'media': ['ë‰´ìŠ¤', 'ì¡ì§€', 'ê¸°ìˆ ì§€', 'ì‚°ì—…']
            }
        }
        
        # è¨€èªåˆ¥ãƒ‰ãƒ¡ã‚¤ãƒ³å‚¾å‘
        self.language_domains = {
            'japanese': ['.jp', '.co.jp', '.or.jp', '.ac.jp'],
            'german': ['.de', '.at', '.ch'],
            'french': ['.fr', '.be', '.ch'],
            'chinese': ['.cn', '.com.cn', '.edu.cn'],
            'korean': ['.kr', '.co.kr'],
            'italian': ['.it'],
            'spanish': ['.es'],
            'russian': ['.ru']
        }
        
    def search_multilingual(self, language: str, query_type: str = 'basic') -> List[Dict]:
        """æŒ‡å®šè¨€èªã§ã®è¤‡åˆææ–™æƒ…å ±æºã‚’æ¤œç´¢"""
        if language not in self.multilingual_keywords:
            print(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„è¨€èª: {language}")
            return []
            
        keywords = self.multilingual_keywords[language].get(query_type, [])
        if not keywords:
            return []
            
        # æ¤œç´¢ã‚¯ã‚¨ãƒªæ§‹ç¯‰
        queries = []
        
        if language == 'japanese':
            queries = [
                f'"{keywords[0]}" OR "{keywords[1]}" ãƒ‹ãƒ¥ãƒ¼ã‚¹ RSS site:.jp',
                f'"{keywords[2]}" OR "{keywords[3]}" å­¦ä¼š RSS',
                f'"ç‚­ç´ ç¹Šç¶­" OR "ã‚«ãƒ¼ãƒœãƒ³ãƒ•ã‚¡ã‚¤ãƒãƒ¼" æ¥­ç•Œ RSS',
                f'è¤‡åˆææ–™ å”ä¼š OR å­¦ä¼š RSS'
            ]
        elif language == 'german':
            queries = [
                f'"{keywords[0]}" OR "{keywords[1]}" Nachrichten RSS site:.de',
                f'"Faserverbund" Forschung RSS',
                f'"Kohlenstofffaser" Industrie RSS'
            ]
        elif language == 'french':
            queries = [
                f'"{keywords[0]}" actualitÃ©s RSS site:.fr',
                f'"fibre de carbone" industrie RSS',
                f'composite aÃ©ronautique RSS'
            ]
        elif language == 'chinese':
            queries = [
                f'"{keywords[0]}" æ–°é—» RSS site:.cn',
                f'"ç¢³çº¤ç»´" è¡Œä¸š RSS',
                f'å¤åˆææ–™ å­¦ä¼š RSS'
            ]
        elif language == 'korean':
            queries = [
                f'"{keywords[0]}" ë‰´ìŠ¤ RSS site:.kr',
                f'"íƒ„ì†Œì„¬ìœ " ì‚°ì—… RSS',
                f'ë³µí•©ì¬ë£Œ í•™íšŒ RSS'
            ]
        
        all_results = []
        for query in queries:
            print(f"\næ¤œç´¢ä¸­ ({language}): {query}")
            results = self.search_google(query, num_results=5)
            all_results.extend(results)
            time.sleep(2)  # APIåˆ¶é™å¯¾ç­–
            
        return all_results
    
    def search_google(self, query: str, num_results: int = 5) -> List[Dict]:
        """Googleæ¤œç´¢å®Ÿè¡Œ"""
        if not self.google_api_key or not self.google_cx:
            return []
            
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            'key': self.google_api_key,
            'cx': self.google_cx,
            'q': query,
            'num': num_results
        }
        
        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            return data.get('items', [])
        except Exception as e:
            print(f"Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def detect_language_from_domain(self, url: str) -> str:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰è¨€èªã‚’æ¨å®š"""
        domain = urlparse(url).netloc.lower()
        
        for language, domains in self.language_domains.items():
            if any(domain.endswith(d) for d in domains):
                return language
                
        return 'english'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def evaluate_multilingual_relevance(self, url: str, title: str, snippet: str, language: str) -> float:
        """è¨€èªåˆ¥ã®é–¢é€£åº¦è©•ä¾¡"""
        score = 0.0
        text = f"{title} {snippet} {url}".lower()
        
        if language not in self.multilingual_keywords:
            return 0.0
            
        # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        for category, keywords in self.multilingual_keywords[language].items():
            category_score = 0.2 if category == 'basic' else 0.1
            for keyword in keywords:
                if keyword.lower() in text:
                    score += category_score
                    
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¨€èªä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
        detected_lang = self.detect_language_from_domain(url)
        if detected_lang == language:
            score += 0.3
            
        return min(score, 1.0)
    
    def find_rss_feeds(self, url: str) -> List[str]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰æ¤œå‡ºï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰"""
        feeds = []
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # RSSãƒªãƒ³ã‚¯æ¤œå‡º
            rss_links = soup.find_all('link', type=['application/rss+xml', 'application/atom+xml'])
            for link in rss_links:
                href = link.get('href')
                if href:
                    feed_url = urljoin(url, href)
                    feeds.append(feed_url)
            
            # è¨€èªåˆ¥ã®ä¸€èˆ¬çš„ãªRSSãƒ‘ã‚¹
            common_paths = ['/rss', '/feed', '/rss.xml', '/feed.xml', '/atom.xml']
            
            # æ—¥æœ¬èªã‚µã‚¤ãƒˆç‰¹æœ‰ã®ãƒ‘ã‚¹
            if self.detect_language_from_domain(url) == 'japanese':
                common_paths.extend(['/news/rss', '/info/rss', '/topics/rss'])
            
            for path in common_paths:
                feed_url = urljoin(url, path)
                if self.validate_feed(feed_url):
                    feeds.append(feed_url)
                    
        except Exception as e:
            print(f"RSSæ¤œå‡ºã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            
        return list(set(feeds))
    
    def validate_feed(self, feed_url: str) -> bool:
        """ãƒ•ã‚£ãƒ¼ãƒ‰æœ‰åŠ¹æ€§ç¢ºèª"""
        try:
            response = self.session.get(feed_url, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                return len(feed.entries) > 0
        except:
            pass
        return False
    
    def discover_multilingual_sources(self):
        """å¤šè¨€èªã®è¤‡åˆææ–™æƒ…å ±æºã‚’ç™ºè¦‹"""
        target_languages = ['japanese', 'german', 'chinese', 'korean']  # å„ªå…ˆè¨€èª
        
        all_discovered = {}
        
        for language in target_languages:
            print(f"\n{'='*50}")
            print(f"ğŸŒ {language.upper()} æƒ…å ±æºã®æ¤œç´¢é–‹å§‹")
            print(f"{'='*50}")
            
            # åŸºæœ¬æ¤œç´¢
            results = self.search_multilingual(language, 'basic')
            
            language_sources = []
            for result in results:
                site_url = result.get('link', '')
                title = result.get('title', '')
                snippet = result.get('snippet', '')
                
                # é–¢é€£åº¦è©•ä¾¡
                relevance = self.evaluate_multilingual_relevance(site_url, title, snippet, language)
                if relevance < 0.3:
                    continue
                    
                print(f"\nèª¿æŸ»ä¸­: {title}")
                print(f"URL: {site_url}")
                print(f"é–¢é€£åº¦: {relevance:.2f}")
                
                # RSSãƒ•ã‚£ãƒ¼ãƒ‰æ¤œå‡º
                feeds = self.find_rss_feeds(site_url)
                
                for feed_url in feeds:
                    source = {
                        'name': title or urlparse(site_url).netloc,
                        'urls': [feed_url],
                        'site_url': site_url,
                        'language': language,
                        'relevance_score': relevance,
                        'discovered_at': datetime.now().isoformat()
                    }
                    language_sources.append(source)
                    print(f"  â†’ RSSãƒ•ã‚£ãƒ¼ãƒ‰ç™ºè¦‹: {feed_url}")
            
            all_discovered[language] = language_sources
            print(f"\n{language}: {len(language_sources)} ä»¶ã®æƒ…å ±æºã‚’ç™ºè¦‹")
        
        return all_discovered
    
    def save_multilingual_candidates(self, sources_by_language: Dict[str, List[Dict]]):
        """å¤šè¨€èªå€™è£œã‚’DBã«ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        total_db_saved = 0
        
        for language, sources in sources_by_language.items():
            if not sources:
                continue
                
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰
            filename = f"multilingual_sources_{language}_{timestamp}.json"
            filepath = os.path.join(os.path.dirname(__file__), '..', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(sources, f, ensure_ascii=False, indent=2)
                
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            db_saved = 0
            for source in sources:
                try:
                    # å€™è£œãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
                    candidate_data = {
                        'name': source.get('name', 'Unknown'),
                        'domain': self.extract_domain(source.get('site_url', '')),
                        'urls': source.get('urls', []),
                        'site_url': source.get('site_url', ''),
                        'category': 'unknown',
                        'language': language,
                        'country_code': self.detect_country_from_language(language),
                        'relevance_score': source.get('relevance_score', 0.5),
                        'discovery_method': 'weekly_multilingual_discovery',
                        'metadata': {
                            'discovered_at': source.get('discovered_at', ''),
                            'source_type': 'multilingual_search',
                            'search_language': language,
                            'feeds_found': len(source.get('urls', []))
                        }
                    }
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ã§UPSERTï¼‰
                    existing = supabase.table('source_candidates').select('id').eq('domain', candidate_data['domain']).execute()
                    
                    if existing.data:
                        # æ—¢å­˜å€™è£œã‚’æ›´æ–°
                        result = supabase.table('source_candidates').update({
                            'relevance_score': candidate_data['relevance_score'],
                            'metadata': candidate_data['metadata']
                        }).eq('domain', candidate_data['domain']).execute()
                        print(f"  æ›´æ–°: {candidate_data['name']} ({candidate_data['domain']})")
                    else:
                        # æ–°è¦å€™è£œã‚’æŒ¿å…¥
                        result = supabase.table('source_candidates').insert(candidate_data).execute()
                        print(f"  è¿½åŠ : {candidate_data['name']} ({candidate_data['domain']})")
                        db_saved += 1
                        
                except Exception as e:
                    print(f"  ã‚¨ãƒ©ãƒ¼: {source.get('name', 'Unknown')} - {str(e)}")
                    
            print(f"\n{language}: {len(sources)} ä»¶ã®å€™è£œã‚’ç™ºè¦‹ â†’ {filename}")
            print(f"  - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜: {db_saved} ä»¶")
            total_db_saved += db_saved
        
        # çµ±è¨ˆæƒ…å ±
        total_sources = sum(len(sources) for sources in sources_by_language.values())
        print(f"\nç·è¨ˆ: {total_sources} ä»¶ã®å¤šè¨€èªæƒ…å ±æºå€™è£œã‚’ç™ºè¦‹")
        print(f"  - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜: {total_db_saved} ä»¶")
        print(f"  - ç®¡ç†ç”»é¢ã§ç¢ºèªãƒ»æ‰¿èªã—ã¦ãã ã•ã„")
        
        print("\nè¨€èªåˆ¥å†…è¨³:")
        for language, sources in sources_by_language.items():
            if sources:
                print(f"  {language}: {len(sources)} ä»¶")
                
    def extract_domain(self, url: str) -> str:
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º"""
        from urllib.parse import urlparse
        try:
            parsed = urlparse(url)
            return parsed.netloc or url
        except:
            return url
            
    def detect_country_from_language(self, language: str) -> str:
        """è¨€èªã‹ã‚‰å›½ã‚³ãƒ¼ãƒ‰ã‚’æ¨å®š"""
        mapping = {
            'japanese': 'JP',
            'german': 'DE',
            'french': 'FR',
            'chinese': 'CN',
            'korean': 'KR',
            'italian': 'IT',
            'spanish': 'ES',
            'russian': 'RU'
        }
        return mapping.get(language, 'unknown')

def main():
    # ãƒ­ã‚°è¨˜éŒ²ç”¨ã®å¤‰æ•°åˆæœŸåŒ–
    start_time = time.time()
    log_data = {
        "task_name": "Weekly Multilingual Source Discovery",
        "task_type": "weekly_multilingual_discovery",
        "sources_processed": 0,
        "articles_found": 0,
        "articles_added": 0,
        "errors_count": 0,
        "details": {
            "languages_searched": [],
            "search_queries": 0,
            "api_calls": 0,
            "relevant_sites": 0,
            "sites_with_rss": 0,
            "candidates_by_language": {},
            "errors": []
        }
    }
    
    try:
        discoverer = MultilingualSourceDiscoverer()
        
        if not discoverer.google_api_key:
            error_msg = "Google APIèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            log_data["errors_count"] += 1
            log_data["details"]["errors"].append(error_msg)
            print(f"è­¦å‘Š: {error_msg}")
            print("å¤šè¨€èªæ¤œç´¢ã«ã¯Google Custom Search APIãŒå¿…è¦ã§ã™")
            return
        
        print("å¤šè¨€èªè¤‡åˆææ–™æƒ…å ±æºã®æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™...")
        print("å¯¾è±¡è¨€èª: æ—¥æœ¬èªã€ãƒ‰ã‚¤ãƒ„èªã€ä¸­å›½èªã€éŸ“å›½èª")
        
        sources = discoverer.discover_multilingual_sources()
        discoverer.save_multilingual_candidates(sources)
        
        # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
        total_candidates = sum(len(lang_sources) for lang_sources in sources.values())
        log_data["articles_added"] = total_candidates
        log_data["details"]["languages_searched"] = list(sources.keys())
        log_data["details"]["candidates_by_language"] = {lang: len(sources_list) for lang, sources_list in sources.items()}
        
    except Exception as e:
        log_data["errors_count"] += 1
        log_data["details"]["errors"].append(str(e))
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        
    finally:
        # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
        end_time = time.time()
        log_data["duration_seconds"] = int(end_time - start_time)
        log_data["status"] = "failed" if log_data["errors_count"] > 0 else "success"
        
        # detailsã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
        log_data["details"] = json.dumps(log_data["details"], ensure_ascii=False)
        
        # task_logsãƒ†ãƒ¼ãƒ–ãƒ«ã«è¨˜éŒ²
        try:
            log_result = supabase.table("task_logs").insert(log_data).execute()
            if hasattr(log_result, "error") and log_result.error:
                print(f"ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {log_result.error}")
            else:
                print(f"å®Ÿè¡Œãƒ­ã‚°è¨˜éŒ²å®Œäº†: {log_data['task_name']}")
                print(f"  - ç™ºè¦‹å€™è£œæ•°: {log_data['articles_added']}")
                print(f"  - å®Ÿè¡Œæ™‚é–“: {log_data['duration_seconds']}ç§’")
                print(f"  - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {log_data['status']}")
        except Exception as e:
            print(f"ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()